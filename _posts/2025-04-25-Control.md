---
title: "Some basic knowledge in control engineering"
date: 2025-04-25
category: "Control"
---

## Laplace transform
### Laplace transform
\\\[F(s) = L[f(t)] = \int_0^{\inf}f(t)e^{-st}dt\\\], where \\\(s = \sigma+\omega i\\\), i.e., \\\(s\\\) is a complex number. \\\(s\\\) is a complex variable in frequency domain and \\\(t\\\) is a real variable in time domain. 

The inverse Laplace transform is given by the following complex integral:
\\\[f(t)=L^{-1}[F] (t)=\frac{1}{2\pi i}\lim_{T\rightarrow \inf}\int_{\gamma-iT}^{\gamma+iT}e^{st}F(s)ds\\\], where \\\(\gamma\\\) is a real number so that the contour path of integration is in the region of convergence of \\\(F(s)\\\). 
The Laplace transform's key property is that it converts differentiation and integration in the time domain into multiplication and division by \\\(s\\\) in the Laplace domain.

The Laplace transform \\\(L[f(t)]\\\) exists if it has expontential order and \\\(\int_0^b|f(t)|dt\\\) exists for any \\\(b>0\\\). OR
The Laplace transform \\\(L[f(t)]\\\) exists if:
1. \\\(f(t)\\\) has expontential order and 
2. on every closed interval \\\([0,b]\\\)
    1. \\\(f(t)\\\) is bounded, 
    2. \\\(f(t)\\\) is piecewise continuous, and
    3. \\\(f(t)\\\) has at most a finite number of discontinuities.

### Properties and theorems
Linearity: 
\\\[L[af(t)+bg(t)] = aL[f(t)]+bL[g(t)]\\\]. 

Derivative:
\\\[L[f^{(n)}(t)] = s^n L(f(t))-\sum_{k=1}^{n}s^{n-k}f^{k-1}(0)\\\].

Integeration:
\\\[L[\int_0^tdt\int_0^tdt...\int_0^tf(t)dt] = \frac{1}{s^n}L[f(t)]\\\]. 
\\\[\int_s^{\inf}L[f(t)]ds = L[\frac{f(t)}{t}]\\\].

Time shifting:
\\\[L[f(t-\tau)u(t-\tau)] = e^{-\tau s}F(s)\\\], where \\\(u(t)\\\) is the Heaviside step function.
\\\[L[e^{at}f(t)]=L[f(t)]\\\]

Convolution:
\\\[L[f(t)*g(t)] = L[f(t)]\cdot L[g(t)]\\\]

## Coding basic control blocks in simulation 
In control system simulations, complex controllers (like PID controllers) are built by connecting simple fundamental blocks. Each block has a specific mathematical behavior and role.

## Gain block
### Concept
A gain block multiplies an input signal by a constant factor (the gain). It scales the amplitude of the signal up or down. 

### Mathematical Equation
If \\\(K\\\) is the gain constant and \\\(x(t)\\\) is the input, then the output \\\(y(t)\\\) is \\\(y(t)=K\cdot x(t)\\\).  There is no differential equation (no dynamics) for a pure gain since it’s an algebraic scaling.

### Transfer Function
In the Laplace domain, a gain is simply \\\(G(s) = K\\\).  This means the output-to-input ratio is \\\(K\\\) for all frequencies (a flat frequency response).

### Role in Control Systems
Gain blocks are ubiquitous in controllers. In a PID controller, the proportional term is essentially a gain block (the proportional gain \\\(K_p\\\) multiplying the error signal). 

### Code implementation
{% highlight ruby %}
def gain_block(yi,p):
    yo = p * yi
    return yo
{% endhighlight %}

## Integrator Block
### Concept
An integrator block produces an output that is the time integral of its input. 

### Mathematical Equation
In continuous time, the integrator is described by a differential equation: \\\(y(t)=\int_0^tx(\tau)d\tau+y(0)\\\), where \\\(y(0)\\\) is the initial output.

### Transfer Function
The Laplace domain transfer function of an integrator (with zero initial condition) is: \\\(G(s)=\frac{Y(s)}{X(s)} = \frac{1}{s}\\\). The \\\(\frac{1}{s}\\\) transfer function has a pole at the origin (s = 0), indicating the integrator’s accumulating behavior. An integrator provides a 90° phase lag (–90°) and its gain decreases with frequency (it acts like a low-pass element, amplifying low-frequency/steady components since as \\\(s\rightarrow 0, \frac{1}{s}\\\) is large).

### Role in Control Systems
In PID control, the integrator block is responsible for the integral action. Its role is to eliminate steady-state errors: by accumulating the error over time, it drives the long-term average error to zero. For example, if a system has a constant offset, the integrator will ramp up its output until the offset is corrected. However, integrators can introduce sluggishness and overshoot if overused, and they can suffer from wind-up. Wind-up means if the controller output saturates (hits a physical limit) but the integrator continues to integrate the error, the accumulated value can become very large. When the saturation condition ends, this “wound-up” integrator can cause an overshoot. To mitigate this, practical PID controllers use techniques like integrator clamping or reset (often involving a limiter on the integrator state – see Limiter block below – to implement anti-windup).

### Code implementation
{% highlight ruby %}
# yo/yi = K / sT
def int_block(h,x0,yi,p):
    f = yi * p[0] / p[1]
    x1 = x0 + h * f
    yo = x1      
    return yo, x1, f
{% endhighlight %}

## Lag Block (First-Order Lag)
### Concept
A lag block (often called a first-order lag or first-order low-pass filter) is a dynamic block that smooths and delays the response of the output relative to changes in the input. Conceptually, it does not allow the output to instantly jump to follow the input; instead, the output lags behind the input. If the input changes suddenly, the lag block’s output will rise or fall gradually, typically in an exponential fashion. At steady state (after enough time), a first-order lag will eventually make its output equal to its input (assuming a constant input), so it has unity DC (steady-state) gain. This block is analogous to a resistor-capacitor (RC) filter in electronics, which smooths a signal.

### Mathematical Equation
A first-order lag is characterized by a time constant \\\(\tau\\\) which dictates how quickly it responds. The differential equation is commonly written as: \\\(\tau \frac{d}{dt}y(t)= x(t) - y(t)\\\). This equation says the rate of change of y is proportional to the difference between the current input and the current output.  This shows that if y is lower than the input x, y will rise (because x−y is positive), and if y is higher than x, y will fall, until eventually y equals x in steady state. 

### Transfer Function
The transfer function of a first-order lag (with unity steady-state gain) is: \\\(G(s)=\frac{Y(s)}{X(s)} = \frac{1}{1+s\tau}\\\). This has a single pole at \\\(s=-\frac{1}{\tau}\\\). At low frequency (s small), \\\(G(s)\sim 1\\\) (so steady-state gain is 1). At high frequency (|s| large), \\\(G(s)\sim \frac{1}{s\tau}\\\), which tends toward 0 — meaning fast changes in the input are attenuated (filtered out). In the time domain, a step input to this transfer function yields an exponential response.

### Role in Control Systems
Lag blocks are often used to model and simulate the inherent dynamics of physical processes or sensors which cannot respond instantaneously (e.g. a thermometer lagging behind temperature changes). In control, they are also used deliberately to filter signals. For example, in a PID controller, a derivative action is very sensitive to noise, so a first-order lag (low-pass filter) is often applied to the derivative term to create a filtered derivative. This is effectively adding a lag to avoid responding to very rapid fluctuations (noise). In terms of classical control design, a phase-lag compensator (not to be confused with just any lag filter) is sometimes implemented to improve steady-state accuracy by introducing a pole near the origin (with a nearby zero to limit the phase impact). The lag block here is the fundamental form of those ideas — it smooths the controller’s action.

### Code implementation
{% highlight ruby %}
# yo/yi = K / (1 + sT)
def lag_block(h,x0,yi,p):   
    f = (yi - x0) / p[1]
    x1 = x0 + h * f
    yo = p[0] * x1    
    return yo, x1, f
{% endhighlight %}

## Lead-Lag Block
### Concept
A lead-lag block combines the characteristics of a lead compensator and a lag compensator in one transfer function. It has one zero and one pole, giving it the ability to both accelerate the response for fast changes (lead effect) and smooth or stabilize the response for slow changes (lag effect). In essence, a lead-lag network can provide phase lead at some frequencies and phase lag at others. If configured as a lead compensator, the block produces an output that anticipates the input changes (a phase-advanced response, somewhat like a differentiator with a steady-state gain). If configured as a lag compensator, it produces an output that is a delayed, smoothed version of the input (phase-delayed, like a filter with higher steady-state gain for accuracy). The general lead-lag block is very flexible and is widely used in control design to shape the system response. 

### Mathematical Equation
A typical continuous-time lead-lag compensator can be represented by the transfer function: \\\(G(s) = \frac{1+sT_{lead}}{1+sT_{lag}}\\\), where \\\(T_{lead}\\\) and \\\(T_{lag}\\\) are time constants for the zero (lead) and pole (lag) respectively. Expanding this into a differential equation in the time domain, we get: \\\(T_{lag}\frac{d}{dt}y(t)+y(t)=T_{lead}\frac{d}{dt}x(t)+x(t)\\\). This equation shows that the output \\\(y(t)\\\) depends on both the instantaneous input \\\(x(t)\\\) and their derivatives. If \\\(T_{lead}>T_{lag}\\\), the term involving \\\(\frac{d}{dt}x(t)\\\) is more influential (yielding a stronger “lead” or anticipative action). If \\\(T_{lead}< T_{lag}\\\), the output changes more slowly relative to the input (a stronger “lag” action). Often, the design is such that at steady state the gain is 1 (if \\\(T_{lead}= T_{lag}\\\) or an appropriate gain factor is included) so that a constant input yields equal constant output.

### Transfer Function
As given above \\\(G(s) = \frac{1+sT_{lead}}{1+sT_{lag}}\\\). This single zero–single pole transfer function will have a zero at \\\(s=-\frac{1}{T_{lead}}\\\) and a pole at \\\(s=-\frac{1}{T_{lag}}\\\). The frequency response of this network can boost certain frequency ranges and attenuate others. For instance, a lead compensator (zero closer to the origin than the pole) can add phase lead and increase high-frequency gain (like a differentiator, improving transient response), whereas a lag compensator (pole closer to origin) adds phase lag and increases low-frequency gain (improving steady-state accuracy, similar to an integrator effect).

### Role in Control Systems
Lead-lag blocks are important in control system tuning and design. They can be used to implement refined versions of PID controllers or other compensators. For example, the derivative action in a PID can be seen as a lead element (it provides anticipation by responding to the rate of change of error), but in practice a pure differentiator is not physical and is noisy. So, a lead-lag approximation is used: the derivative is implemented as a lead compensator with a small lag to filter out high-frequency noise. Also, many PID controllers in industry have an equivalent “lead-lag” form: the PI part introduces a pole at zero (integrator) and a zero to cancel it out partially, and the derivative part introduces a zero and a high-frequency pole. In summary, a lead-lag block can act as a building block for custom controllers: a lead portion to improve stability margins and speed up response, and a lag portion to ensure zero steady-state error or desired low-frequency gain. 

### Code implementation
{% highlight ruby %}
# yo/yi = (1 + sTa) / (1 + sTb)   
def leadlag_block(h,x0,yi,p):  
    f = (yi - x0) / p[1]
    x1 = x0 + h * f
    yo = x1 + p[0] * (yi - x0) / p[1]
    return yo, x1, f
{% endhighlight %}

## Limiter Block (Saturation)
### Concept
A limiter block (often called a saturation block) restricts a signal to a specified range. Conceptually, it “clamps” the output so it cannot exceed a maximum value or go below a minimum value. Any input beyond those bounds gets limited to the nearest bound. This block represents the physical limits of actuators or variables in simulation (no infinite or unbounded outputs in a real system). The output of a limiter is a piecewise-linear function of the input – linear (slope 1) within the allowed region, flat (slope 0) once the limit is hit.

### Mathematical Equation
The limiter’s behavior can be described piecewise: If we denote the lower limit as \\\(L_{min}\\\) and upper limit as \\\(L_{max}\\\), and \\\(u(t)\\\) is the input, the output \\\(y(t)\\\) is:
\\\[ y(t) = 
\begin{cases} 
    L_{max} \& \text{if } u(t) >= L_{max}, \\\\ 
    L_{min} \& \text{if } u(t)<= L_{min}, \\\\ 
    u(t) \& \text{if } L_{min}< u(t)< L_{max} 
\end{cases}
\\\].


## References
[1] https://en.wikipedia.org/wiki/Laplace_transform \
[2] https://zhuanlan.zhihu.com/p/128812880 \
[3] https://zhuanlan.zhihu.com/p/152647974 \
[4] https://www.cs.ucr.edu/~craigs/ucla-courses/135.2.16s/laplace-existence.pdf \
[5] https://ctms.engin.umich.edu/CTMS/?aux=Extras_Blocklib#:~:text=Gain \
[6] https://amperelabs.com.au/wp-content/uploads/Dynamic-modelling-workshop_v1.1-1.pdf \
[7] https://www.tutorialspoint.com/control_systems/control_systems_compensators.htm#:~:text=From%20the%20above%20equation%2C%20%24,is%20always%20greater%20than%20one \